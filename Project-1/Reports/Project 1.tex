\documentclass[amssymb,twocolumn,aps]{revtex4-2}

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
%\usepackage [norsk]{babel} %if you write norwegian
\usepackage[english]{babel}  %if you write english

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\usepackage{graphicx}         % include graphics such as plots
\usepackage[table]{xcolor}
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing 
\usepackage{float}	 % force placement of tables and figures



\begin{document}
	
\title{Project 1 \\
    \normalsize FYS-STK4155}
\date{\today}               
\author{Selma Beate Øvland}
\affiliation{University of Oslo}

\newpage
	
\begin{abstract}

We study polynomial regression on the noisy Runge function with added Gaussian noise, and compare Ordinary Least Squares (OLS), Ridge, and Lasso across closed-form and gradient-based solvers. Here we analyze: (i) model complexity (polynomial degree), (ii) explicit regularization (Ridge/LASSO) vs implicit regularization from early stopping in gradient descent (GD), (iii) optimizers (Momentum, AdaGrad, RMSProp, Adam), and (iv) stochastic mini-batch training. We evaluate performance using test MSE and $R^2$, convergence and divergence, and the bias–variance trade-off via the bootstrap. 

Our main findings: (1) explicit $\ell_2$ shrinkage (Ridge) gives the strongest generalization at high degrees; (2) for OLS, early-stopped GD can improve on the closed-form solution because of implicit regularization; (3) for Ridge, the closed-form solution remains best unless we tune GD and run the model long enough; (4) adaptive optimizers mainly speed up convergence rather than change final test error on these convex problems; (5) mini-batch training shows the expected noisier curves per epoch with modest test-MSE gaps that shrink with more epochs or tuned steps. The bootstrap study reproduces the classic U-shape, where bias falls and variance rises with the degree, and Ridge/LASSO reduces the variance.

\end{abstract}


\maketitle

\section{Introduction}
A challenge in machine learning methods for regression is to obtain a reliable input-output relationship from data while at the same time controlling the model complexity to avoid poor generalization. While polynomials can approximate smooth functions well, high degree models are prone to numerical instability and overfitting when data is noisy or features are not properly scaled. For effective regression models we want to balance simplicity and flexibility to minimize the expected prediction error. The bias-variance trade-off summarizes this critical tension. \cite{compfys39}. \\

A model with high bias suffers from underfitting, failing to capture the complexity of the true function. A model with high variance suffers from overfitting, meaning it has learnt the noise of the training data, leading to poor generalization on unseen data \cite{compfys38}, \cite{hastie}. Our goal in this project is to explore methods that balance this trade-off, as shown in figure \ref{fig:g_ols}. To obtain models that are both stable and accurate, we make use of regularization and resampling techniques. Especially for multicollinearity issues, we benefit from Ridge and Lasso regression, as they introduce a penalty term to the MSE cost function, forcing the model parameters towards smaller magnitudes, which reduces variance. \\

In the project, we will use the Runge function.

\begin{equation}
    f(x) = \frac{1}{1 + 25x^2}
    \label{eq:runge}
\end{equation}


To create a synthetic dataset of evenly spaced data points. We use this function to study how fitting it with high-polynomials illustrates the issue with interpolation instability and overfitting, which is a known phenomena useful for studying regularization methods \cite{wikipedia-runge}. \\

We first look at fitting OLS up to degree 15 on the Runge function in section \ref{subsubsec:ols_method}, where we evaluate the accuracy with MSE and $R^2$ vs degree. In section \ref{subsubsec:rdg_method}, we study the penalty term $\lambda$ and its relationship to the polynomial degree again using MSE and $R^2$ analysis. In part (c-f), we replace the equations with gradient-based solvers and examine convergence under different step sizes. In part (g), we implement bootstrap to separate the error into bias and variance, to explore the effect that the polynomial degree and sample size will have. Part (h) implements k-fold cross-validation and compares these test errors to the bootstrap estimates. Ultimately, we will have a method for diagnosing underfitting and overfitting, as well as selecting polynomial degrees and penalty terms that yield optimal results. 

\section{Theory and Methods}

\subsection{Theory}
\label{subsection:theory}
Regression analysis assumes that the output data can be represented by a continous function f(x) plus noise. In linear regression this function is approximated linearly with 

\begin{equation}
    \overline{y} = X \theta
\end{equation}
This is a supervised regression problem where we assume that the true data is generated from a noisy model expressed as: 

\begin{equation}
y = f(x)+\varepsilon
\end{equation}

where in this project the true signal f(x) is expressed as the Runge function \ref{eq:runge} and $\varepsilon$ represents the random normal distributed noise. 

The input domain for x is [-1,1] with additive Gaussian noise. 
\subsubsection{Ordinary Least Squares (OLS)}
\label{subsubsec:ols_theory}

The standard approach for OLS is to minimize the Mean Squared Error (MSE) cost function $C (\theta)$: 

\begin{equation}
    \min_{\theta\in\mathbb{R}^p} \; C(\theta)
= \frac{1}{n}\,\lVert y - X\theta \rVert_2^2
\end{equation}

Minimizing this function analytically leads to the solution for optimal parameters $\hat{\theta}_{\text{OLS}}$

\begin{equation}
\hat{\theta}_{\text{OLS}} = (X^\top X)^{-1} X^\top y
\end{equation}

The OLS estimator is unbiased, meaning the expectation value $\mathbb{E}\!\left[\hat{\theta}_{\text{OLS}}\right]$ equals the true parameter value $\theta$. The existence of a solution to OLS depends on invertibility of the Hessian matrix $H=X^TX$. Problems can arise if the design matrix X is high dimensional or if its columns are linearly dependent (known as super-collinearity), potentially leading to a singular (non-invertible) $X^TX$ matrix. \\

To address the issues described above we can use regularized regression like Ridge and Lasso. They add a penalty term to the standard OLS cost function. \\

\subsubsection{Ridge regression ($\ell_2$ regularization)}
\label{subsubsec:rdg_theory}

Ridge regression adds an $\ell_2$ norm penalty to the OLS cost function, controlled by a hyperparameter $\lambda \ge 0$

\begin{equation}
\min_{\theta \in \mathbb{R}^p}
\quad
\frac{1}{n}\,\lVert y - X\theta \rVert_2^2
\;+\;
\lambda \lVert \theta \rVert_2^2
\end{equation}

We find the optimal Ridge parameters $\hat{\theta}_{\text{Ridge}}$ analytically: 

\begin{equation}
    \hat{\theta}_{\text{Ridge}}
= (X^\top X + \lambda I)^{-1} X^\top y
\end{equation}

Adding $\lambda I$ ensures the matrix $X^\top X + \lambda I)$ is invertible for finite $\lambda$, solving the singularity issues OLS regression can have. In contrast to OLS, the Ridge estimator is biased for any $\lambda \ge 0$, but we also reduce variance. \\

\subsubsection{Lasso regression ($L_1$ regularization)}
\label{subsubsec:lasso_theory}

The $\ell_1$ term is classified as 

\begin{equation}
\min_{\theta \in \mathbb{R}^p}
\quad
\frac{1}{n}\,\lVert y - X\theta \rVert_2^2
\;+\;
\lambda \lVert \theta \rVert_1
\end{equation}

where $\lVert \theta \rVert_1 = \sum_{i=1}^p \lvert \theta_i \rvert$. \\

The Lasso cost function normally does not yield an analytical solution. We need to solve it numerically, typically with variants of gradient descent methods. A key difference between Lasso and Ridge, is that where Ridge shrinks some of the coefficiants towards zero, Lasso will reduce some of them entirely to zero. This means that where Ridge keeps all the coefficients, Lasso only keeps a subset of them. This can be an advantage if we want sparser models (and if the true signal is sparse). However, Ridge has an advantage when the true signal is denser and we have multicollinearity. Using Lasso in the wrong way can result in underfitting and oversimplification. 

\subsubsection{Optimization algorithms}
\label{subsubsec:opt_theory}

\textit{4.1 Convergence and divergence} \\

The MSE cost function of both OLS and Ridge are convex functions, where for a convex function any local minimum is also a global minimum, guaranteeing that the Gradient Descent (GD) cost function is also a convex function if the learning rate is sufficiently small. \\

\textit{4.2 Gradient descent (GD), fixed learning rate $\eta$} \\

Plain GD iteratively updates the parameters $\theta$ by taking a step proportional to the negative gradient of the cost function $C(\theta)$: 

\begin{equation}
\theta_{k+1} \;=\; \theta_k \;-\; \eta \,\nabla_{\theta} C(\theta_k)
\end{equation}

Simply put, the step size is the same for every parameter. In ML models the full gradient $\nabla_{\theta} C(\theta)$ must be computed by summing over all datapoints in a dataset. Calculating this full gradient is computationally expensive when the datasets become large. \\ 

\textit{4.3 Stochastic gradient descent (SGD)} \\

With stochastic gradient descent the cost function $C(\theta)$ can be expressed as a sum over the individual losses $c_i(x_i,\theta)$ for N data points:

\begin{equation}
    C(\theta) = \frac{1}{N}\sum_{i=1}^{N} c_i(x_i,\theta)
\end{equation}

SGD introduces randomness and computational efficiency by approximating the full gradient $\nabla_{\theta} C(\theta)$ by summing over a subset of the data $B_k$: 

\begin{equation}
    \theta_{j+1}
= \theta_j - \eta_j \sum_{i \in B_k} \nabla_{\theta}\, c_i(x_i,\theta_j)
\end{equation}

\textit{4.4 Momentum and Adaptive Methods (AdaGrad, RMSProp, ADAM) } \\

These methods modify the learning rate $\eta$ to speed up convergence: 

\begin{itemize}
    \item Momentum: Keeps a running average of past gradients and moves in that direction. 
    \item AdaGrad: Each parameter has its own learning rate. If the parameter sees big gradients, shrink the learning rate; and if it rarely changes, increase the learning rate. 
    \item RMSProp: Similar to AdaGrad, but uses an exponential moving average of squared gradients, to avoid AdaGrad's learning rate goes to zero problems. Works well on noisy problems.
    \item ADAM: Combines momentum and RMSProp. Tracks average gradient (momentum) and average squared gradient (RMSProp), then bias-corrects them early on. It is fast and stable on noisy and complex data, and is a solid default.
\end{itemize}

\subsubsection{Evaluation metrics}
\label{subsubsec:eval_theory}

\textit{5.1 Mean squared error (MSE)} \\

MSE is the squared difference between the true values ($y_i$) and the model's predicted values ($\overline{y_i}$). It's squared to punish the big differences more than the small ones, and to avoid positives/negatives canceling each other out. The lower the value, the better (smaller error). \\

Its mathematical expression is 

\begin{equation}
    \text{MSE}(y, \tilde{y}) = \frac{1}{n} \sum_{i=0}^{n-1} (y_i - \tilde{y}_i)^2 
\end{equation}

It's best to use MSE to optimize and see the absolute error size. MSE is sensitive to outliers. \\

\textit{5.2 $R^2$ score function} \\

The $R^2$ score function represents how much better your model is than just measuring the mean of the training data. It's represented as a quantile, where 1 equals perfect predictions and 0 means no better predictions than the mean, so a high value is better. Definition: 

\begin{equation}
    R^2 \;=\; 1 \;-\; \frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{\sum_{i=1}^n (y_i - \bar y_{\text{train}})^2}
\end{equation}

We use $R^2$ as a measure of how well our model fits the data. 

\subsubsection{Bias-Variance tradeoff}
\label{subsubsec:bias_var_theory}

The MSE can be decomposed into three fundamental components: 

\begin{equation}
    \mathbb{E}\!\big[(y-\tilde y)^2\big]
= \operatorname{Bias}^2[\tilde y]
+ \operatorname{Var}[\tilde y]
+ \sigma^2
\end{equation}

\begin{itemize}
    \item Bias squared ($\operatorname{Bias}^2[\tilde y]$): Represents the error due to simplifying assumptions in the model (e.g., modeling a complex non-linear function using a low-order polynomial). 
    \item Variance ($\operatorname{Var}[\tilde y]$): Measures how much the predicted values $\overline{y}$ changes over different training data sets. Highly flexible models usually have high variance. 
    \item Irreducible error ($\sigma^2$): The variance of the noise inherent in the data, which cannot be reduced by the model. 
\end{itemize}

The Bias-Variance trade-off represents the relationship between a model's complexity (bias) and its sensitivity to the training data (variance). 
A simple model (low complexity) has high bias and low variance. 
A complex model (high flexibility) has low bias and high variance, adapting too closely to the noise in the training data. With Ridge and Lasso, we introduce a small bias, but in return, we achieve a greater reduction in variance, usually resulting in lower expected test error (MSE) than with OLS. 

\subsubsection{Generalization error estimators}
\label{subsubsec:error_theory}

\textit{7.1 Hold out}

Split the data randomly into training and test data sets. For each model complexity (polynomial degree or regularization factor $\lambda$), we first fit the model on the training set, then compute the test and training MSE. As complexity increases, the train MSE usually decreases, while the test MSE exhibits a characteristic U-shape: high error from underfitting at low complexity, a good fit in the middle, and overfitting at high complexity. This helps us choose the complexity that minimizes test MSE. Understanding the relationship between test errors, train errors, and model complexity is fundamental to understanding the bias-variance trade-off. 

\textit{7.2 Bootstrap}

We use our training dataset and re-draw samples from it (with replacement), several times. We then fit our model to the new datasets to see how predictions change. For each test point, we compute the average prediction, variance, MSE, and squared bias. This tells us if our model is stable (low variance) or touchy (high variance) as we nudge the data. 

\textit{7.3 K-fold Cross-Validation (CV)}

K-fold CV is a resampling technique designed to avoid issues where random splitting might lead to an unbalanced influence of certain samples on model building or evaluation. We split the data into K equal parts (or folds). For each setting (degree or regularization parameter) we train on K-1 fold, and evaluate on the left-out fold. We repeat this so every fold is the test set once. Average over the K-errors to get CV-MSE. This way, we train and split the data more than once, which helps if the data is sparse. Using CV-MSE, we can select the optimal complexity and retrain with the chosen setting.  

\subsubsection{Scaling, centering, and data splitting}
\label{subsubsec:scal_theory}

Standardizing or rescaling data is crucial for numerical stability and model performance, especially when the design matrix X contains features on drastically different scales, which can occur when using high-degree polynomials (as columns with high polynomials would either explode or vanish). Placing all features on a comparable scale prevents large-scale features from dominating the $X^TX$ matrix. In Ridge or Lasso, where we penalize the size of the coefficients, we need a comparable scale so the strength of the regularization parameter $\lambda$ has a consistent meaning across features. \\

We split the data into training and testing data. The purpose of the testing data is to simulate unseen data to evaluate a model's performance. It is essential to split the data before performing any standardization, as this keeps the test data "hidden" from the model. We only standardize the training data (this action let's the model "see" the data, which is why we don't do it on the test data). After fitting the transformation parameters to the training data, we apply the fitted transformation to both the test and training matrices. The test set must be transformed using the mean and standard deviation derived from the training set, not from the test set itself. When we use K-fold CV, this principle extends to each fold. When using a fold as the test set, the standardization parameters must be calculated from the other folds (the K-1 folds). 

\subsection{Methods - Implementation}
\label{section:methods}

We will perform a regression analysis by implementing the stepwise methods discussed in the Theory section for several polynomial degrees. Through all our tests, we will use the Runge function \eqref{eq:runge}, and we will evaluate the methods using MSE and $R^2$, in addition to convergence loss curves where applicable. 


\subsubsection{Generating our data set and preprocessing}
 \label{subsubsec:data_method}

We study supervised regression of noisy observations 

\begin{equation}
    y = f(x) + \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0,\sigma^2)
\end{equation}

where $f(x)$ is the Runge function \eqref{eq:runge} on [-1,1]. In our code, synthetic data is generated using the \texttt{make\_data(n=300, noise\_sd=0.3, seed=42)} function in data.py. It generates n input points uniformly spaced in the [-1, 1] interval, evaluates the Runge function at these points, and adds Gaussian noise, resulting in two arrays as the output: x (inputs) and y (targets). We fix the seed (\texttt{seed=42})throughout the calculations so the results are reproducible by using the function \texttt{ rng = np.random.default\_rng(seed) }.  \\

To give the model the flexibility to capture the shape of the curve, we expand each scalar input (x) into polynomial features using \texttt{build\_features(x,\ degree=p,\ include\_bias=False)} function, where the output is our design matrix. This produces columns with $x, x^2, x^3 .. x^p$ where include\_bias=False, means we omit the intercept column. Each column represents the complexity we have chosen for our model. We handle the intercept separately by centering the training targets and adding back the mean at prediction time. We do this because the intercept column contributes nothing to the standardizations, and we don't want Ridge/Lasso to penalize the intercept column. After that, we split the dataset into training and testing sets. The StandardScaler function is fitted on the training matrix. Then, we scale both the training and test matrices using the training mean and standard deviation. We also center the training targets by subtracting their training mean. This is the procedure for standardization that we will use throughout the project, except when we implement k-fold CV, where we need to tweak our preprocessing procedure slightly to avoid data leakage. In K-fold CV we fit all preprocessing only on the training fold, so we apply the fitted transform to the validation fold. We achieve this by placing the feature construction function, scaling, and centering within a "pipeline" (a scikit-learn object) so that they are refit within each fold. \\

\subsubsection{Ordinary Least Square (OLS) for the Runge function}
 \label{subsubsec:ols_method}

We start with the simplest closed-form model: OLS. The OLS finds a set of coefficients that matches the model's predictions as close as possible to the observed targets. In our implementation (\texttt{ fit\_ols}), we use the data set generated and preprocessing techniques described in Section \ref{subsubsec:data_method}, and solve the normal equations directly. The output from this are the parameters $\theta$. We calculate the MSE and $R^2$ to evaluate the model's fit across different polynomial degrees. We expect the method to perform more poorly as the polynomial degree increases. We also plot how the noise affects MSE and $R^2$. We expect a noise free model to work much better at higher polynomials than if we implement noise. We plot a noise-free and heavy noise scenario with MSE vs $R^2$ to see the effect. For the MSE we expect a U-shaped curve where the intermediate polynomial degrees are the best fit, and for the $R^2$ we expect the same effect, with an upside down U-shaped curve, where the low degrees and higher degrees are bad fits. We will also plot the parameters $\theta$ vs polynomial degree. We expect the parameter size to blow up as the degree increases. FIX \\

To improve the performance of our model with high-degree polynomials, we implement the $L_2$ norm penalty, also known as Ridge regularization. \\

\subsubsection{Adding Ridge regression for the Runge function}
 \label{subsubsec:rdg_method}

Data and polynomial features are produced by the same utilities explained in Section \ref{subsubsec:ols_method}, still using Runge as the target function. In \texttt{def fit\_ridge(X, y\_c, lam, n\_factor=True)} we add the $L_2$ term to the $X^TX$ term from OLS, so this becomes $X^TX + \lambda I$, which reduces variance at the cost of small bias, but improves on MSE. We expect that the coefficients will shrink with the $L_2$ penalty. 

\subsubsection{GD}

We implement a fixed learning rate Gradient Descent for OLS and Ridge, where we replace the closed-form solution with 

\[
\nabla_{\theta}\mathcal{L}_{\text{OLS}} = \frac{1}{n}\,X^{\top}(X\theta - y_c),
\qquad
\nabla_{\theta}\mathcal{L}_{\text{Ridge}} = \frac{1}{n}\,X^{\top}(X\theta - y_c) + \alpha\,\theta.
\]

Where $\alpha = \lambda/n$ to keep GD consistent with the closed-form solutions. We initialize and standardize the data in the same way as before. We compare our GD solutions to the closed-form solutions by measuring MSE and $R^2$. In addition, we visualize convergence using loss vs. iteration curves. 

\subsubsection{Optimizers, Lasso, SGD}

On top of plain GD we tried the four optimizers: momentum, RMSProp, AdaGrad, and Adam to update the learning rate. We use the same data initialization and standardization as before; the only difference is the update rule for the learning rate. 

For Lasso 

\subsubsection{Bootstrap}

\subsubsection{K-fold Cross-Validation}

We evaluated polynomial regression on the Runge data with noise $\sigma=0.3$ and degrees $p=1,\dots,15$.
For each degree we computed a $10$-fold cross-validation (CV) MSE for:
\begin{itemize}
    \item \textbf{OLS},
    \item \textbf{Ridge} (with $\lambda$ tuned inside each training fold on a small logarithmic grid; the fold’s prediction uses the best $\lambda$),
    \item \textbf{Lasso} (same tuning strategy for $\lambda$).
\end{itemize}
We also compared CV against a bootstrap estimate of the test MSE (using the same train/test split as earlier) with bootstrap size $B=200$.


\subsection{Use of AI tools}
 \label{subsubsec:ai_method}
	
\subsubsection{ChatGPT}
I used ChatGPT to help me make a plan for how to structure the code for this project. Before that, I started with part (a), wrote the code, then went to part (b) and realized I could reuse some code from part (a). I ended up with messy code without clear structure, with repeated variables and functions more times than needed. I wanted to separate the code and make it reusable for all parts of this project, and also for other projects/exercises later. So I uploaded the project description to ChatGPT and asked for a plan on how to organize the code (not the actual code). ChatGPT gave me a plan where I can make one Python file at a time (data, models, plotting, etc.), so I don’t have to think only in terms of part a, b, c, which was a bit overwhelming. This helps me keep the code cleaner and easier to reuse. \\

\subsubsection{Grammarly}

I have a Grammarly business account through my work that I use for language correction (grammar, spelling, fixing sentences, punctuation, etc). 
	
\section{Results and Discussion}\label{section:results} 

\subsection{Results}

\paragraph{Setup:}
Data from the Runge function
\[
f(x)=\frac{1}{1+25x^{2}}, \qquad x\in[-1,1],
\]
with additive Gaussian noise \(\varepsilon\sim\mathcal N(0,\sigma^{2})\).
For $\sigma = 0.3$  we construct polynomial features up to
degree \(p\le 15\), use a single train/test split, apply standardization
on the training data only, fit closed-form OLS on the centered target
\(y_{\text{train}}-\bar y_{\text{train}}\), and shift predictions back to the
original scale by adding \(\bar y_{\text{train}}\).

\subsubsection{Simplest CF method: Ordinary Least Square (OLS)}

We tested two different noise scenarios shown in Figure \ref{fig:ols_vs_degree}: a noiseless dataset and a high noise dataset. For the noiseless scenario test MSE decreases monotonically with degree p and approaches $\sim 0$. $R^2$ rises toward $\sim 1$. Higher degree improves fit. For a high noise dataset (sd=0.7) MSE is high and flattens out in particular areas, then rises sharply with complexity grade towards 1. $R^2$ fluctuates as well, but decreases sharply with complexity towards 0. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/runge_ols_mse_r2_vs_degree.png}
    \caption{Test MSE vs polynomial degree, for different noise values.}
    \label{fig:ols_vs_degree}
\end{figure}

In figure \ref{fig:norms_vs_degree} and beyond, we have settled on a noise level between our two tests in figure \ref{fig:ols_vs_degree}, sd = 0.3, to replicate real data. In \ref{fig:norms_vs_degree} we plot the coefficient size vs degree p, and see that it remains modest at low to medium values of p, then grows rapidly after p $\approx$12. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/runge_ols_theta_norms_vs_degree.png}
    \caption{The figure shows the relationship between the $\ell_2$ norm of the OLS coefficients and the polynomial degree p. $\ell_2$ norm of the coefficients grows quickly with degree.}
    \label{fig:norms_vs_degree}
\end{figure}

\subsubsection{Introducing the penalty term $\lambda$, CF Ridge regression}

In figure \ref{fig:ridge_mse_lambda}, as the penalty term $\lambda$ increases from minimal values, the test MSE is smallest at around $\lambda \approx 10^{-4}-10^{-5}$ and then increases steadily from there. $R^2$ has its peak at the same region ($10^{-4}-10^{-5}$) and steadily decreases from there. In figure \ref{fig:ridge_theta_norms}, the coefficient norm shrinks steadily as $\lambda$ increases. Shrinkage is most effective at $10^{-6}-10^{-3}$ then flattens out. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/runge_ridge_mse_r2_vs_lambda.png}
    \caption{Test MSE and $R^2$ plotted against different $\lambda$ values.}
    \label{fig:ridge_mse_lambda}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/ridge_theta_norms_vs_lambda.png}
    \caption{The figure shows the relationship between the $l_2$ norm of the Ridge coefficients and $\lambda$. $\ell_2$ is shrinking as $\lambda$ grows, eventually flattening once the coefficients are relatively small.}
    \label{fig:ridge_theta_norms}
\end{figure}

\subsubsection{Gradient descent (plain/full batch)}

Table \ref{tab:test-ols-ridge} shows the MSE and $R^2$ evaluations from GD with a fixed learning rate compared to our closed-form methods. Overall Ridge (CF) had the best performance metrics. For OLS specifically, GD performed better, but for Ridge, CF had better metrics. 

\begin{table}[h!]  % [t] or [h!] as you prefer
\caption{Test performance for OLS and Ridge (CF = closed form, GD = gradient descent).}
\label{tab:test-ols-ridge}
\begin{ruledtabular}
\begin{tabular}{llcc} % no spaces here
Model & Solver & MSE & $R^2$ \\
\colrule
OLS   & CF & 0.135149 & 0.321477 \\
OLS   & GD & 0.131579 & 0.339400 \\
Ridge & CF & 0.126253 & 0.366140 \\
Ridge & GD & 0.132170 & 0.336433 \\
\end{tabular}
\end{ruledtabular}
\end{table}



Table \ref{tab:train-ols-ridge} shows the printed training loss we calculated from GD and CF. The difference between them should ideally be $\approx$ 0 for each model; here, both methods have a difference in their training loss. A larger GD loss value indicates that GD didn't fully converge.  

\begin{table}[h!]
\caption{Training loss for OLS and Ridge (CF = closed form, GD = gradient descent).}
\label{tab:train-ols-ridge}
\begin{ruledtabular}
\begin{tabular}{lcc}
Model & Loss (CF) & Loss (GD) \\
\colrule
Ridge & 0.042513 & 0.045991 \\
OLS   & 0.040149 & 0.045871 \\
\end{tabular}
\end{ruledtabular}
\end{table}



\subsubsection{Optimizers, Lasso, SGD}

In Figure \ref{fig:loss_curves_gd}, GD and momentum cluster pretty tightly for OLS, while RMSProp and Adam drop faster, but RMSProp flattens out earlier, while Adam keeps descending. For Ridge, all optimizers reduce the loss more quickly, but improve the most in terms of momentum from OLS. AdaGrad is performing the worst on both models, even more than the baseline GD. \\

In Figure \ref{fig:loss_curves_sgd}, we have the same loss curves as for GD. AdaGrad is still the worst-performing optimizer. We see a lot more noise in the mini-batch curves, especially for plain SGD and momentum, but all curves descend faster than the curves in figure \ref{fig:loss_curves_gd}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/Part_c_d_e_f_GD_optimizers_loss_curves.png}
    \caption{Loss vs iteration curves for plain GD and with optimizers, showing convergence.}
    \label{fig:loss_curves_gd}
\end{figure}

 \begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/Part_f_SGD_all_optimizers_loss_curves.png}
    \caption{Loss vs iteration curves for plain SGD and with optimizers, showing convergence.}
    \label{fig:loss_curves_sgd}
\end{figure}

Test metrics for GD are shown in table \ref{tab:test-metrics-optimizers}. All optimizers reach a comparable test performance. Adam is performing the best for both OLS and Ridge, while AdaGrad has the worst scores. For SGD in Table \ref{tab:mb-optimizers-1000}, we have a worse overall MSE than the GD variants, where AdaGrad underperforms for both models. 

\begin{table}[h!]
\caption{Test metrics, GD baseline with optimizer for OLS and Ridge.}
\label{tab:test-metrics-optimizers}
\begin{ruledtabular}
\begin{tabular}{lcccc}
Optimizer & \multicolumn{2}{c}{OLS} & \multicolumn{2}{c}{Ridge} \\
\colrule
 & MSE & $R^2$ & MSE & $R^2$ \\
\colrule
gd       & 0.132044 & 0.3371 & 0.131033 & 0.3421 \\
momentum & 0.132831 & 0.3331 & 0.131375 & 0.3404 \\
adagrad  & 0.137117 & 0.3116 & 0.137484 & 0.3098 \\
rmsprop  & 0.130348 & 0.3456 & 0.131679 & 0.3389 \\
adam     & 0.128056 & 0.3571 & 0.130480 & 0.3449 \\
\end{tabular}
\end{ruledtabular}
\end{table}


% --- Mini-batch test metrics by optimizer (OLS vs Ridge) ---
\begin{table}[h!]
\caption{Test metrics, SGD baseline, OLS and Ridge with optimizers.}
\label{tab:mb-optimizers-1000}
\begin{ruledtabular}
\begin{tabular}{lcccc} 
Optimizer & \multicolumn{2}{c}{OLS (mini-batch)} & \multicolumn{2}{c}{Ridge (mini-batch)} \\
\colrule
 & MSE & $R^2$ & MSE & $R^2$ \\
\colrule
sgd       & 0.142868 & 0.2827 & 0.140808 & 0.2931 \\
momentum  & 0.143477 & 0.2797 & 0.139641 & 0.2989 \\
adagrad   & 0.155100 & 0.2213 & 0.155068 & 0.2215 \\
rmsprop   & 0.146386 & 0.2651 & 0.146255 & 0.2657 \\
adam      & 0.146501 & 0.2645 & 0.146678 & 0.2636 \\
\end{tabular}
\end{ruledtabular}
\end{table}

Lasso variants, both scikit-learn and GD, with optimizers, have more variance in their results. In table \ref{tab:lasso-variants}, FISTA and scikit-learn Lasso get the best scores, while our ISTA/GD Lasso needs to be fine-tuned to achieve more competitive metrics. 

\begin{table}[h!]
\caption{Test metrics for Lasso variants. (SKL = scikit-learn Lasso.)}
\label{tab:lasso-variants}
\begin{ruledtabular}
\begin{tabular}{lcc}
Method & MSE & $R^2$ \\
\colrule
ISTA/GD    & 0.153787 & 0.2279 \\
SKL     & 0.129139 & 0.3517 \\
PGD     & 0.137124 & 0.3116 \\
FISTA   & 0.128733 & 0.3537 \\
RMSProp & 0.134195 & 0.3263 \\
Adam    & 0.133778 & 0.3284 \\
\end{tabular}
\end{ruledtabular}
\end{table}


\subsubsection{Bias-Variance Trade-off}

Figure \ref{fig:fig2.11} shows a figure similar to Fig. 2.11 of Hastie, Tibshirani, and Friedman. This plot was created using a single train/test split and shows that the train MSE falls monotonically with the degree, while the test MSE forms a shallow U-shape and reaches a minimum at around 12 degrees. 

Figure \ref{fig:g_ols} shows a bootstrap version of the decomposed Bias-Variance, where $Bias^2$ first decreases as the polynomial degree increases, but then grows a little. The variance increases slightly as p also increases. Test MSE grows as p increases. The minimum here is between 8 and 12 degrees. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/fig2.11_style.png}
    \caption{Training and test MSE on a single train/test split.  }
    \label{fig:fig2.11}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/g_ols_bias_variance_bootstrap.png}
    \caption{}
    \label{fig:g_ols}
\end{figure}

\subsubsection{K-fold Cross-Validation}

All models in Figure \ref{fig:10_fold_CV} show a substantial decrease in CV-MSE until model complexity reaches between 9 and 12, where it plateaus. Around p = 13, Ridge achieves a slight improvement over OLS; otherwise, they remain comparable. Lasso is tied with Ridge until the plateau is reached. The shaded region appears to widen up to the plateau and then remains unchanged. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/cv_mse.png}
    \caption{10-fold Cross-Validation: [OLS vs Ridge vs Lasso] vs polynomial degree p. Lines show mean MSE with CV. The shaded region is the $\pm$ 1 standard deviation across folds, only for OLS.}
    \label{fig:10_fold_CV}
\end{figure}

In figure \ref{fig:CV_vs_bootstrap}, both curves have a minimum around the same degree, around p = 10. The bootstrap curve has slightly higher values,  

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Project-1/Figures/OLS_cv_vs_bootstrap.png}
    \caption{}
    \label{fig:CV_vs_bootstrap}
\end{figure}

\begin{table}[h!]
\caption{Best model complexity by method (10-fold CV). Bootstrap row uses the same fixed train/test split as earlier and is not directly comparable to CV.}
\label{tab:best-degrees-cv}
\begin{ruledtabular}
\begin{tabular}{lccc}
Method & Best degree $p^\ast$ & CV MSE  \\
\colrule
OLS                                & 15 & 0.098184 \\
Ridge (best $\alpha$ per degree)   & 14 & 0.098207 \\
Lasso (best $\alpha$ per degree)   & 10 & 0.102341  \\
Bootstrap OLS (fixed split)        & 15 & 0.119178 \\
\end{tabular}
\end{ruledtabular}
\end{table}


\subsection{Discussion}

For the closed-form Ordinary Least Squares (OLS) method, these two curves reproduce the expected bias-variance trade-off. Increasing p reduces bias, but beyond a certain point, the variance dominates, so the test error stops improving and actually gets worse. In figure \ref{fig:ols_vs_degree} we see that a noiseless prediction function behaves well at higher polynomial degrees. This is a pure bias regime, with noiseless data higher-degree polynomials approximate the Runge curve better. But for the model with heavy noise we see the more characteristic U-shape we predicted in the method section, meaning that our model gives the best predictions at medium complexity and becomes unstable at higher and lower degrees.

In figure \ref{fig:norms_vs_degree} we see the ill-conditioning of high-complexity design matrices using the OLS method. Large coefficients are a classic sign of overfitting, which is why we proceed to test the Ridge method with its penalty term. \\

Ridge with its penalty term trades variance for bias. At small $\lambda$ we have low bias and higher variance, and this is where the model performs best on this split with p = 15. As we increase the penalty term, the coefficients are strongly shrunk and variance falls, but bias grows so test MSE and $R^2$ worsens. The curve for test MSE in \ref{fig:ridge_mse_lambda} is mostly monotone after its minimum at $10^{-4}-10^{-5}$, further shrinking the coefficients after this will add bias. \\

Next we replaced the analytical solution that we used for the closed-form OLS and Ridge, with a plain gradient descent with a fixed learning rate. We still used the same data, with the same noise, same polynomial degree p = 15 and the same penalty term we found worked well for the Ridge method earlier. For Ridge in GD form we still use the $\alpha$-form, as in $\alpha=\frac{\lambda}{n}$. 

In table \ref{tab:test-ols-ridge} the results from GD are compared to the CF metrics. For OLS we see that GD attains a slightly lower MSE score than CF despite a higher training loss, as seen in table \ref{tab:train-ols-ridge}, meaning we most likely stopped the GD before reaching full convergence. On Ridge, the CF solution performs best. Since Ridge already shrinks the coefficients with the chosen $\lambda$, we get extra under-fitting with GD. We see that if $\eta$ is too small, GD converges slowly and can stop far from the optimum. If $\eta$ is too large, GD can oscillate or diverge. We also printed the training loss for CF and GD; we see that they don't match (as they should, indicating that GD has converged completely).

Our results show that the chosen $\eta$ and iteration number yielded only partial convergence. For OLS, it acted helpfully, but for Ridge, we need a tuned $\eta$ or iteration number to close the gap between Gd and CF training loss. We may need millions of steps to make GD converge, as GD is slow on ill-conditioned problems, such as high-degree Runge polynomials. Increasing $\lambda$ also speeds up the convergence. \\

Adding optimizers to our GD approach changed the behaviour more than it changed the metrics. Momentum and Adam reached low trainig loss in fewer iterations than plain GD, while AdaGrad and RMSProp need to be tuned better. We attempted to adjust $\eta$ for each optimizer, as one setting doesn't work for all of them; however, selecting the correct one requires further investigation. For metrics, the MSE and $R^2$ values ended up fairly similar between the different optimizers, differing primarily in how quickly and stably the methods converged. \\

Introducing Lasso with optimizers, we saw that ¿??

Instead of full-batch GD, we test mini-batch SGD. Mini-batches introduce gradient noise, which speeds up progress but yields a noisier curve. We observe that the full-batch GD yields a smoother curve; however, the number of iterations has been significantly reduced in SGD. Efficiency has been improved in SGD, but we have increased variance. Tuning the number of epochs and $\eta$ we can probably match the metrics of GD, and we could have used a stop-function to validate our results. \\

For the Bias-Variance trade-off 





\section{Conclusion}\label{section:conclusion} 

For low-noise cases and low to medium complexity, OLS works best. When introducing Ridge, we see a useful operating range of around $10^{-4}$ for this exact data, split, and degree, where Ridge matches OLS but keeps coefficients smaller. However, too little shrinkage yields overly large coefficients, and too much shrinkage underfits (resulting in high MSE and low $R^2$). Overall, Ridge stabilizes the degree 15 polynomial and offers a controllable bias-variance trade-off. Plain GD with a fixed learning rate works, but is computationally inefficient for high-degree polynomials. The learning rate $\eta$ matters, but a higher $\eta$ means more progress per step, but limits speed. With optimizers, we can achieve the same metrics more quickly and with fewer computations; however, if we don't tune $\eta$ properly, we can get divergence. 
SGD is computationally effective, and by tuning $\eta$ and the number of epochs, we can achieve comparable test metrics to full-batch GD, which is significantly more computationally expensive. Which regularization method we use matters for flexible models. Lasso/Ridge, tuned by CV, matched or improved upon OLS at higher degrees by controlling the variance. With bootstrap methods, we could predict an optimal degree for our data. Using K-fold Cross-Variance, we confirm the degree range and can select the sensible $\lambda$ value for Ridge and Lasso. We're left with a recipe to follow for how to optimize our methods for the future: 

\begin{itemize}
    \item Pick the optimal degree by K-fold CV
    \item Tune $\lambda$ by CV. Use Ridge if the data is dense, Lasso if the data is sparse. 
    \item Use Adam (gave us mostly the better results in our test above). 
    \item Bootstrap can be used to demonstrate the bias-variance trade-off. 
\end{itemize}


\bibliography{biblio}

\end{document}
