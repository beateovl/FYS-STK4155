{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bb3979",
   "metadata": {},
   "source": [
    "\n",
    "# Project 1 — Parts c)–f)\n",
    "This notebook solves **c, d, e, f** using the provided modules in this repo:\n",
    "`data.py`, `grad.py`, `metrics.py`, `models.py`, `plots.py`, `resampling.py`, `utils.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3023a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, pathlib\n",
    "\n",
    "# Project-1 folder (parent of this Notebooks/ folder)\n",
    "proj_dir = pathlib.Path.cwd().parent\n",
    "if str(proj_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_dir))\n",
    "\n",
    "# Figures folder\n",
    "fig_dir = proj_dir / \"Figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from Code.data import make_data, build_features, split_and_scale\n",
    "\n",
    "# --- Project-wide experiment config (used in ALL notebooks) ---\n",
    "\n",
    "# Data settings\n",
    "N_SAMPLES   = 300     # number of samples\n",
    "NOISE_SD    = 0.3     # noise standard deviation\n",
    "SEED_DATA   = 42      # data generation\n",
    "SEED_SPLIT  = 42      # train/test split (split_and_scale uses this)\n",
    "DEG_MAX     = 15      # max polynomial degree for features\n",
    "P_FIXED     = 15      # fixed polynomial degree for regularization experiments\n",
    "TEST_SIZE   = 0.20    # train/test split size\n",
    "\n",
    "\n",
    "# Regularization grids\n",
    "LAM_GRID_RIDGE = np.logspace(-6, 1, 40)  # wider range for ridge\n",
    "LAM_GRID_LASSO = np.logspace(-6, 0, 60)  # less range for LASSO\n",
    "\n",
    "# Optimizer settings\n",
    "ETA_FULL   = 5e-3     # full-batch GD steps\n",
    "ITERS_FULL = 5000     # full-batch GD iterations\n",
    "LAM_RIDGE  = 0.0203   # Taken from ridge experiments (02_Ridge_lambda.ipynb) best lambda for degree 15\n",
    "BETA = 0.3    # momentum parameter\n",
    "B1 = 0.9    # Adam parameter\n",
    "B2 = 0.999  # Adam parameter\n",
    "EPS = 1e-8  # Adam parameter\n",
    "RHO = 0.99  # Adadelta parameter\n",
    "\n",
    "# Mini-batch SGD\n",
    "EPOCHS_MB    = 25     # passes over data\n",
    "BATCH_SIZE   = 64     # minibatch size\n",
    "ETA_MB_OLS   = 1e-2   # for OLS, can be larger\n",
    "ETA_MB_RIDGE = 1e-2   # for ridge, need to tune\n",
    "SEED_MB      = 42     # minibatch shuffle\n",
    "\n",
    "# --- Generate data explicitly---\n",
    "x, y = make_data(n=N_SAMPLES, noise_sd=NOISE_SD, seed=SEED_DATA)\n",
    "X_full = build_features(x, degree=DEG_MAX, include_bias=False)\n",
    "\n",
    "\n",
    "X_tr_s, X_te_s, y_tr_c, y_te, scaler, y_mean = split_and_scale(X_full, y, test_size=TEST_SIZE, random_state=SEED_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545c8fd",
   "metadata": {},
   "source": [
    "\n",
    "## c) Gradient Descent for OLS and Ridge (using `grad.py`)\n",
    "\n",
    "We first use the plain gradient descent method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5677a2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS  — Test MSE: 0.15016, R²: 0.2461\n",
      "Ridge (λ=0.0203) — Test MSE: 0.15017, R²: 0.2461\n"
     ]
    }
   ],
   "source": [
    "from Code.metrics import mse, r2\n",
    "from Code.models import predict_centered\n",
    "from Code.grad import gd, grad_ridge, grad_ols\n",
    "\n",
    "# Standardized design matrix, p is number of features\n",
    "p = X_tr_s.shape[1]  \n",
    "\n",
    "# OLS via gradient descent\n",
    "theta_ols = gd(X_tr_s, y_tr_c, eta=ETA_FULL, iters=ITERS_FULL, theta0=np.zeros(p), lam=None) # lam=None → OLS\n",
    "yhat_te_ols = predict_centered(X_te_s, theta_ols, y_mean) # add back y_mean\n",
    "mse_ols = mse(y_te, yhat_te_ols) \n",
    "r2_ols  = r2(y_te, yhat_te_ols) \n",
    "\n",
    "# Ridge via gradient descent (λ given)\n",
    "theta_ridge = gd(X_tr_s, y_tr_c, eta=ETA_FULL, iters=ITERS_FULL, theta0=np.zeros(p), lam=LAM_RIDGE, n_factor=True) # lam=float → Ridge\n",
    "yhat_te_ridge = predict_centered(X_te_s, theta_ridge, y_mean) # add back y_mean\n",
    "mse_ridge = mse(y_te, yhat_te_ridge) \n",
    "r2_ridge  = r2(y_te, yhat_te_ridge)\n",
    "\n",
    "print(f\"OLS  — Test MSE: {mse_ols:.5f}, R²: {r2_ols:.4f}\")\n",
    "print(f\"Ridge (λ={LAM_RIDGE}) — Test MSE: {mse_ridge:.5f}, R²: {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca11a0",
   "metadata": {},
   "source": [
    "\n",
    "## d) Optimizers: Momentum, AdaGrad, RMSProp, Adam\n",
    "\n",
    "We keep the same objectives but change the update rule. We'll use both Ridge and OLS. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746275f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FULL-BATCH (RIDGE, λ=0.0203)\n",
      "gd        MSE=0.15017  R²=0.2461\n",
      "momentum  MSE=0.15017  R²=0.2461\n",
      "adagrad   MSE=0.15077  R²=0.2431\n",
      "rmsprop   MSE=0.13550  R²=0.3197\n",
      "adam      MSE=0.13055  R²=0.3446\n",
      " FULL-BATCH (OLS, λ=None)\n",
      "gd        MSE=0.15016  R²=0.2461\n",
      "momentum  MSE=0.15017  R²=0.2461\n",
      "adagrad   MSE=0.15075  R²=0.2431\n",
      "rmsprop   MSE=0.13517  R²=0.3214\n",
      "adam      MSE=0.12921  R²=0.3513\n"
     ]
    }
   ],
   "source": [
    "# --- Optimizer (no SGD) ---\n",
    "def run_optimizer(X, y, lam=LAM_RIDGE, eta=ETA_FULL, iters=ITERS_FULL, optimizer=\"gd\",\n",
    "                  beta=BETA, eps=EPS, rho=RHO, b1=B1, b2=B2, seed=SEED_DATA):\n",
    "    \"\"\"\n",
    "    Full-batch optimizers for OLS/Ridge:\n",
    "      gd        : plain gradient descent (full batch)\n",
    "      momentum  : GD with momentum\n",
    "      adagrad   : adaptive per-parameter steps\n",
    "      rmsprop   : exponential moving avg of squared grads\n",
    "      adam      : momentum + rmsprop with bias correction\n",
    "    No stochastic SGD here as we'll do that later.\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    rng = np.random.default_rng(SEED_DATA)  # kept for parity; not used in full-batch\n",
    "    theta = np.zeros(p, float)\n",
    "    v = np.zeros_like(theta)  # momentum buffer\n",
    "    s = np.zeros_like(theta)  # second-moment buffer\n",
    "    m = np.zeros_like(theta)  # first-moment buffer\n",
    "    t = 0\n",
    "\n",
    "    for _ in range(ITERS_FULL):\n",
    "        g = grad_ridge(X, y, theta, LAM_RIDGE) if lam is not None else grad_ols(X, y, theta)\n",
    "        t += 1\n",
    "\n",
    "        if optimizer in (\"gd\",):                     # plain full-batch GD\n",
    "            theta -= ETA_FULL * g\n",
    "        elif optimizer == \"momentum\":\n",
    "            v = BETA * v + (1 - BETA) * g\n",
    "            theta -= ETA_FULL * v\n",
    "        elif optimizer == \"adagrad\":\n",
    "            s += g * g\n",
    "            theta -= (ETA_FULL / (np.sqrt(s) + EPS)) * g\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            s = RHO * s + (1 - RHO) * (g * g)\n",
    "            theta -= (ETA_FULL / (np.sqrt(s) + EPS)) * g\n",
    "        elif optimizer == \"adam\":\n",
    "            m = B1 * m + (1 - B1) * g\n",
    "            s = B2 * s + (1 - B2) * (g * g)\n",
    "            m_hat = m / (1 - B1**t)\n",
    "            s_hat = s / (1 - B2**t)\n",
    "            theta -= ETA_FULL * m_hat / (np.sqrt(s_hat) + EPS)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {optimizer}\")\n",
    "    return theta\n",
    "\n",
    "# --- Run comparisons ---\n",
    "opts = [\"gd\", \"momentum\", \"adagrad\", \"rmsprop\", \"adam\"]\n",
    "results = {}\n",
    "\n",
    "for opt in opts:\n",
    "    th = run_optimizer(X_tr_s, y_tr_c, lam=LAM_RIDGE, eta=ETA_FULL, iters=ITERS_FULL, optimizer=opt)\n",
    "    yhat = predict_centered(X_te_s, th, y_mean)\n",
    "    results[opt] = (mse(y_te, yhat), r2(y_te, yhat))\n",
    "\n",
    "print(f\" FULL-BATCH (RIDGE, λ={LAM_RIDGE})\")\n",
    "\n",
    "# Pretty print\n",
    "for k, (mse_v, r2_v) in results.items():\n",
    "    print(f\"{k:8s}  MSE={mse_v:.5f}  R²={r2_v:.4f}\")\n",
    "\n",
    "for opt in opts:\n",
    "    th = run_optimizer(X_tr_s, y_tr_c, lam=None, eta=ETA_FULL, iters=ITERS_FULL, optimizer=opt)\n",
    "    yhat = predict_centered(X_te_s, th, y_mean)\n",
    "    results[opt] = (mse(y_te, yhat), r2(y_te, yhat))\n",
    "\n",
    "print(\" FULL-BATCH (OLS, λ=None)\")\n",
    "\n",
    "# Pretty print\n",
    "for k, (mse_v, r2_v) in results.items():\n",
    "    print(f\"{k:8s}  MSE={mse_v:.5f}  R²={r2_v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3def2",
   "metadata": {},
   "source": [
    "\n",
    "## e) LASSO \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d939db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.061e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.058e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.029e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e+01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.866e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.644e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.385e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.087e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.712e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.659e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.982e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.192e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.290e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.299e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.296e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.364e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e-01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.112e-01, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best by MSE:\n",
      "λ* = 1.00e-06  |  MSE* = 0.129401  |  R²* = 0.3503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.927e-02, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.362e-02, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.723e-02, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# --- LASSO (scikit-learn) on the same split ---\n",
    "from sklearn.linear_model import Lasso\n",
    "from Code.metrics import mse, r2\n",
    "\n",
    "# choose a reasonable λ grid for standardized features\n",
    "\n",
    "\n",
    "Xtr, Xte = X_tr_s[:, :p], X_te_s[:, :p]  # fixed degree p\n",
    "mses, r2s, norms = [], [], []\n",
    "\n",
    "# (optional) warm-start: going from large -> small lambdas is faster\n",
    "for lam in LAM_GRID_LASSO:\n",
    "    model = Lasso(alpha=lam,\n",
    "                  fit_intercept=False,    # we centered y_tr -> intercept handled by y_mean\n",
    "                  max_iter=ITERS_FULL,\n",
    "                  tol=1e-4,\n",
    "                  random_state=SEED_DATA,\n",
    "                  selection=\"cyclic\")\n",
    "    model.fit(Xtr, y_tr_c)\n",
    "    theta = model.coef_.ravel()\n",
    "\n",
    "    # add back the training mean for predictions\n",
    "    yhat = Xte @ theta + y_mean\n",
    "\n",
    "    mses.append(mse(y_te, yhat))\n",
    "    r2s.append(r2(y_te, yhat))\n",
    "    norms.append(float(np.linalg.norm(theta)))\n",
    "\n",
    "mses = np.asarray(mses); r2s = np.asarray(r2s); norms = np.asarray(norms)\n",
    "\n",
    "# Plot test MSE and R² vs λ\n",
    "\n",
    "\"\"\" fig, ax1 = plt.subplots(figsize=(9,5))\n",
    "ax1.plot(lambdas, mses, 'o-', label='Test MSE')\n",
    "ax1.set_xscale('log'); ax1.set_xlabel('λ'); ax1.set_ylabel('MSE (test)'); ax1.grid(True, ls='--', alpha=0.4)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(lambdas, r2s, 's--', color='C2', label='Test R²')\n",
    "ax2.set_ylabel('R² (test)')\n",
    "\n",
    "ax1.set_title(f'LASSO vs λ (degree={p}), noise=0.3')\n",
    "ax1.legend(loc='best')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (optional) coefficient norm diagnostic\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(lambdas, norms, 'x-')\n",
    "plt.xscale('log'); plt.xlabel('λ'); plt.ylabel(r'||θ||₂'); plt.grid(True, ls='--', alpha=0.4)\n",
    "plt.title('LASSO coefficient size vs λ, noise=0.3, degree=15'); plt.tight_layout(); plt.show() \"\"\"\n",
    "\n",
    "best_idx = int(np.argmin(mses))\n",
    "print(\"\\nBest by MSE:\")\n",
    "print(f\"λ* = {LAM_GRID_LASSO[best_idx]:.2e}  |  MSE* = {mses[best_idx]:.6f}  |  R²* = {r2s[best_idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd1bb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #Lasso via (sub)gradient descent with fixed learning rate \"\"\"\n",
    "\n",
    "def lasso_gd(X, y, theta, eta, lmbda, Niterations):\n",
    " \n",
    "    #LASSO via plain (sub)gradient descent with fixed learning rate.\n",
    "\n",
    "   \n",
    "    n = X.shape[0]  # number of samples\n",
    "    for iter in range(Niterations):\n",
    "        # 1) OLS gradient component: (2/n) * X^T (X theta - y)\n",
    "        residual = X @ theta - y\n",
    "        OLS_gradient_component = (2.0 / n) * (X.T @ residual)\n",
    "\n",
    "        # 2) L1 gradient component (subgradient): lambda * sign(theta)\n",
    "        # (note: at theta_i = 0 the subgradient is any value in [-1, 1]; using sign(0)=0 is a common practical choice)\n",
    "        L1_gradient_component = lmbda * np.sign(theta)\n",
    "\n",
    "        # 3) Total gradient\n",
    "        total_gradient = OLS_gradient_component + L1_gradient_component\n",
    "\n",
    "        # 4) Parameter update with fixed learning rate\n",
    "        theta -= eta * total_gradient\n",
    "\n",
    "    return theta \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cd7133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selmabeateovland/Documents/Fys-stk4155/Oppgaver og prosjekt/Project1/FYS-STK4155/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.929e+00, tolerance: 4.234e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO (scikit, λ=2.73e-05) — Test MSE: 0.128600, R²: 0.3544\n",
      "LASSO (GD    , λ=2.73e-05) — Test MSE: 0.149394, R²: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" xgrid = np.linspace(-1, 1, 400)\\nXgrid_full = build_features(xgrid, degree=X_tr_s.shape[1], include_bias=False)  # degree = p features\\nXgrid_s    = scaler.transform(Xgrid_full)[:, :p]\\n\\n# True function for reference\\nfgrid = 1/(1+25*xgrid**2)\\n\\nygrid_sk = Xgrid_s @ theta_sk + y_mean\\nygrid_gd = Xgrid_s @ theta_gd + y_mean\\n\\nplt.figure(figsize=(9,5))\\nplt.plot(xgrid, fgrid, label='True f(x)', lw=2)\\nplt.plot(xgrid, ygrid_sk, label=f'LASSO scikit (λ={lam})')\\nplt.plot(xgrid, ygrid_gd, label=f'LASSO GD (λ={lam})', ls='--')\\nplt.title('LASSO fits on dense grid (degree = p)')\\nplt.xlabel('x'); plt.ylabel('y')\\nplt.grid(True, ls='--', alpha=0.4); plt.legend(); plt.tight_layout()\\nplt.show() \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compare LASSO: scikit vs. GD at λ=2.73e-05 (best lambda value from tests above)---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lam = 2.73e-05  \n",
    "\n",
    "# 1) scikit-learn Lasso \n",
    "lasso_sk = Lasso(alpha=lam, fit_intercept=False, max_iter=20000, tol=1e-4, random_state=42)\n",
    "lasso_sk.fit(X_tr_s, y_tr_c)\n",
    "theta_sk = lasso_sk.coef_.ravel()\n",
    "yhat_sk  = X_te_s @ theta_sk + y_mean\n",
    "mse_sk   = mse(y_te, yhat_sk)\n",
    "r2_sk    = r2(y_te, yhat_sk)\n",
    "\n",
    "# 2) Your GD Lasso at same λ\n",
    "p = X_tr_s.shape[1]\n",
    "theta0 = np.zeros(p)\n",
    "eta = 5e-3\n",
    "Niterations = 3000  # bump iterations a bit for a fairer comparison\n",
    "theta_gd = lasso_gd(X_tr_s, y_tr_c, theta=theta0, eta=eta, lmbda=lam, Niterations=Niterations)\n",
    "yhat_gd  = X_te_s @ theta_gd + y_mean\n",
    "mse_gd   = mse(y_te, yhat_gd)\n",
    "r2_gd    = r2(y_te, yhat_gd)\n",
    "\n",
    "print(f\"LASSO (scikit, λ={lam}) — Test MSE: {mse_sk:.6f}, R²: {r2_sk:.4f}\")\n",
    "print(f\"LASSO (GD    , λ={lam}) — Test MSE: {mse_gd:.6f}, R²: {r2_gd:.4f}\")\n",
    "\n",
    "# ---------- Plot 1: overlay fitted curves on a dense grid ----------\n",
    "\n",
    "\"\"\" xgrid = np.linspace(-1, 1, 400)\n",
    "Xgrid_full = build_features(xgrid, degree=X_tr_s.shape[1], include_bias=False)  # degree = p features\n",
    "Xgrid_s    = scaler.transform(Xgrid_full)[:, :p]\n",
    "\n",
    "# True function for reference\n",
    "fgrid = 1/(1+25*xgrid**2)\n",
    "\n",
    "ygrid_sk = Xgrid_s @ theta_sk + y_mean\n",
    "ygrid_gd = Xgrid_s @ theta_gd + y_mean\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(xgrid, fgrid, label='True f(x)', lw=2)\n",
    "plt.plot(xgrid, ygrid_sk, label=f'LASSO scikit (λ={lam})')\n",
    "plt.plot(xgrid, ygrid_gd, label=f'LASSO GD (λ={lam})', ls='--')\n",
    "plt.title('LASSO fits on dense grid (degree = p)')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.grid(True, ls='--', alpha=0.4); plt.legend(); plt.tight_layout()\n",
    "plt.show() \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48bd6cb",
   "metadata": {},
   "source": [
    "\n",
    "## f) Stochastic Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea63655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-batch Adam (λ=2.73e-05):  MSE=0.130551, R²=0.3446\n",
      "SGD (sum form) Adam (λ=2.73e-05, B=64):  MSE=0.158845, R²=0.2025\n"
     ]
    }
   ],
   "source": [
    "def sgd_minibatch_sum(\n",
    "    X, y, lam=1e-3, epochs=40, batch_size=64, eta=1e-2, optimizer=\"sgd\",\n",
    "    beta=0.9, eps=1e-8, rho=0.99, b1=0.9, b2=0.999, seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Mini-batch SGD (sum form) for OLS/Ridge with per-sample loss:\n",
    "        c_i(θ) = 1/2 (x_i^T θ - y_i)^2 + (lam/2) ||θ||^2\n",
    "    Batch gradient (sum form):\n",
    "        G_B = X_B^T (X_B θ - y_B) + lam * |B| * θ\n",
    "    Update:\n",
    "        θ <- θ - η * G_B\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "    theta = np.zeros(p, float)\n",
    "\n",
    "    # state buffers (for momentum/Adagrad/RMSProp/Adam)\n",
    "    v = np.zeros_like(theta)\n",
    "    s = np.zeros_like(theta)\n",
    "    m = np.zeros_like(theta)\n",
    "    t = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        idx = rng.permutation(n) # shuffle data each epoch\n",
    "        for start in range(0, n, batch_size):\n",
    "            B = idx[start:start+batch_size] # batch indices\n",
    "            Xb, yb = X[B], y[B]\n",
    "            nb = Xb.shape[0]\n",
    "\n",
    "            r = Xb @ theta - yb                            # residuals\n",
    "            g = (Xb.T @ r) + lam * nb * theta              # SUM-FORM gradient\n",
    "\n",
    "            t += 1\n",
    "            if optimizer == \"sgd\":\n",
    "                theta -= eta * g\n",
    "            elif optimizer == \"momentum\":\n",
    "                v = beta * v + (1 - beta) * g\n",
    "                theta -= eta * v\n",
    "            elif optimizer == \"adagrad\":\n",
    "                s += g * g\n",
    "                theta -= (eta / (np.sqrt(s) + eps)) * g\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                s = rho * s + (1 - rho) * (g * g)\n",
    "                theta -= (eta / (np.sqrt(s) + eps)) * g\n",
    "            elif optimizer == \"adam\":\n",
    "                m = b1 * m + (1 - b1) * g\n",
    "                s = b2 * s + (1 - b2) * (g * g)\n",
    "                m_hat = m / (1 - b1**t)\n",
    "                s_hat = s / (1 - b2**t)\n",
    "                theta -= eta * m_hat / (np.sqrt(s_hat) + eps)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown optimizer\")\n",
    "    return theta\n",
    "\n",
    "\n",
    "lam_val = 2.73e-05\n",
    "B0, eta0 = 64, 1e-2\n",
    "B = 128\n",
    "eta = eta0 * (B0 / B)  # scale eta for sum-form\n",
    "epochs = int(1.5 * 25)\n",
    "\n",
    "# full-batch (e.g., Adam)\n",
    "theta_full = run_optimizer(X_tr_s, y_tr_c, lam=lam_val, eta=eta0, iters=5000, optimizer=\"adam\")\n",
    "\n",
    "# stochastic (sum form), same optimizer rule\n",
    "theta_sgd  = sgd_minibatch_sum(X_tr_s, y_tr_c, lam=lam_val, epochs=epochs, batch_size=B, eta=eta, optimizer=\"adam\", seed=42)\n",
    "\n",
    "yhat_full = predict_centered(X_te_s, theta_full, y_mean)\n",
    "yhat_sgd  = predict_centered(X_te_s, theta_sgd,  y_mean)\n",
    "\n",
    "print(f\"Full-batch Adam (λ={lam_val}):  MSE={mse(y_te,yhat_full):.6f}, R²={r2(y_te,yhat_full):.4f}\")\n",
    "print(f\"SGD (sum form) Adam (λ={lam_val}, B={64}):  MSE={mse(y_te,yhat_sgd):.6f}, R²={r2(y_te,yhat_sgd):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ba04d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-batch Adam: 0.13055141405037554 0.3445609716177649\n",
      "Mini-batch Adam: 0.156467557704395 0.21444784998240352\n"
     ]
    }
   ],
   "source": [
    "def sgd_minibatch(\n",
    "    X, y, lam=1e-3, epochs=40, batch_size=64, eta=1e-2, optimizer=\"sgd\",\n",
    "    beta=0.9, eps=1e-8, rho=0.99, b1=0.9, b2=0.999, seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Part f) Mini-batch SGD framework using the SAME update rules as parts c–e.\n",
    "    Matches the 1/(2n) loss convention: grad = (1/nb) X_b^T (X_b θ - y_b) + lam * θ\n",
    "    Supports: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "    theta = np.zeros(p, float)\n",
    "\n",
    "    # state buffers (shared across optimizers)\n",
    "    v = np.zeros_like(theta)  # momentum\n",
    "    s = np.zeros_like(theta)  # second moment (AdaGrad/RMSProp/Adam)\n",
    "    m = np.zeros_like(theta)  # first moment (Adam)\n",
    "    t = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        idx = rng.permutation(n)\n",
    "        for start in range(0, n, batch_size):\n",
    "            b = idx[start:start+batch_size]\n",
    "            Xb, yb = X[b], y[b]\n",
    "            nb = Xb.shape[0]\n",
    "\n",
    "            # gradient for ridge MSE with 1/(2n) convention\n",
    "            r = Xb @ theta - yb\n",
    "            g = (Xb.T @ r) / nb + lam * theta\n",
    "\n",
    "            t += 1\n",
    "            if optimizer == \"sgd\":\n",
    "                theta -= eta * g\n",
    "\n",
    "            elif optimizer == \"momentum\":\n",
    "                v = beta * v + (1 - beta) * g\n",
    "                theta -= eta * v\n",
    "\n",
    "            elif optimizer == \"adagrad\":\n",
    "                s += g * g\n",
    "                theta -= (eta / (np.sqrt(s) + eps)) * g\n",
    "\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                s = rho * s + (1 - rho) * (g * g)\n",
    "                theta -= (eta / (np.sqrt(s) + eps)) * g\n",
    "\n",
    "            elif optimizer == \"adam\":\n",
    "                m = b1 * m + (1 - b1) * g\n",
    "                s = b2 * s + (1 - b2) * (g * g)\n",
    "                m_hat = m / (1 - b1**t)\n",
    "                s_hat = s / (1 - b2**t)\n",
    "                theta -= eta * m_hat / (np.sqrt(s_hat) + eps)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown optimizer: {optimizer}\")\n",
    "\n",
    "    return theta\n",
    "\n",
    "theta_full = run_optimizer(X_tr_s, y_tr_c, lam=2.73e-05, eta=5e-3, iters=5000, optimizer=\"adam\")\n",
    "theta_sgd  = sgd_minibatch(X_tr_s, y_tr_c, lam=2.73e-05, epochs=25, batch_size=64, eta=5e-3, optimizer=\"adam\")\n",
    "\n",
    "yhat_full = predict_centered(X_te_s, theta_full, y_mean)\n",
    "yhat_sgd  = predict_centered(X_te_s, theta_sgd,  y_mean)\n",
    "print(\"Full-batch Adam:\", mse(y_te, yhat_full), r2(y_te, yhat_full))\n",
    "print(\"Mini-batch Adam:\", mse(y_te, yhat_sgd),  r2(y_te, yhat_sgd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d43d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================================\n",
      " LEADERBOARD (lower MSE is better) \n",
      "=======================================================================\n",
      "method                        MSE          R²        ||θ||₂       time[s]\n",
      "Ridge closed-form             0.127581   0.3595   1.522e+01   0.001\n",
      "OLS adam                      0.129212   0.3513   5.359e+00   0.133\n",
      "Ridge adam                    0.130551   0.3446   4.286e+00   0.113\n",
      "OLS closed-form               0.135149   0.3215   1.771e+03   0.005\n",
      "OLS rmsprop                   0.135171   0.3214   3.005e+00   0.076\n",
      "Lasso scikit                  0.135397   0.3202   2.240e+00   0.007\n",
      "Ridge rmsprop                 0.135495   0.3197   2.890e+00   0.086\n",
      "OLS gd                        0.150165   0.2461   5.149e-01   0.093\n",
      "OLS momentum                  0.150165   0.2461   5.149e-01   0.057\n",
      "Ridge gd                      0.150167   0.2461   5.145e-01   0.057\n",
      "Ridge momentum                0.150167   0.2461   5.145e-01   0.069\n",
      "OLS adagrad                   0.150755   0.2431   6.355e-01   0.061\n",
      "Ridge adagrad                 0.150765   0.2431   6.310e-01   0.074\n",
      "OLS rmsprop (mini-batch)      0.151749   0.2381   4.181e-01   0.002\n",
      "Ridge rmsprop (mini-batch)    0.151758   0.2381   4.159e-01   0.002\n",
      "Ridge adam (mini-batch)       0.153667   0.2285   3.938e-01   0.003\n",
      "OLS adam (mini-batch)         0.153686   0.2284   3.954e-01   0.003\n",
      "OLS adagrad (mini-batch)      0.159233   0.2006   1.655e-01   0.002\n",
      "Ridge adagrad (mini-batch)    0.159235   0.2006   1.654e-01   0.002\n",
      "OLS sgd (mini-batch)          0.162539   0.1840   1.111e-01   0.003\n",
      "Ridge sgd (mini-batch)        0.162545   0.1839   1.111e-01   0.002\n",
      "OLS momentum (mini-batch)     0.163102   0.1811   1.073e-01   0.002\n",
      "Ridge momentum (mini-batch)   0.163107   0.1811   1.073e-01   0.002\n",
      "\n",
      "Best by MSE: Ridge closed-form (MSE=0.127581, R²=0.3595)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== Leaderboard (now includes mini-batch rows) =====\n",
    "\n",
    "# config\n",
    "optimizers_full = [\"gd\", \"momentum\", \"adagrad\", \"rmsprop\", \"adam\"]   # full-batch\n",
    "optimizers_mb   = [\"sgd\", \"momentum\", \"adagrad\", \"rmsprop\", \"adam\"]  # mini-batch\n",
    "eta_ols_full    = 5e-3\n",
    "eta_ridge_full  = 5e-3\n",
    "iters_full      = 5000\n",
    "lam_ridge       = 1e-3\n",
    "lam_lasso       = 1e-3\n",
    "\n",
    "# mini-batch hyperparams\n",
    "epochs_mb    = 25\n",
    "batch_size   = 64\n",
    "eta_mb_ols   = 1e-2\n",
    "eta_mb_ridge = 1e-2\n",
    "seed_mb      = 42\n",
    "\n",
    "Xtr, Xte = X_tr_s, X_te_s  # ensure you've sliced to degree p earlier if needed\n",
    "\n",
    "results = []\n",
    "\n",
    "def add_result(name, theta, yhat, t0):\n",
    "    results.append({\n",
    "        \"method\":  name,\n",
    "        \"MSE\":     float(mse(y_te, yhat)),\n",
    "        \"R2\":      float(r2(y_te, yhat)),\n",
    "        \"||θ||2\":  float(np.linalg.norm(theta)),\n",
    "        \"time_s\":  time.perf_counter() - t0\n",
    "    })\n",
    "\n",
    "# --- OLS: closed form ---\n",
    "t0 = time.perf_counter()\n",
    "theta_ols_cf = np.linalg.pinv(Xtr) @ y_tr_c\n",
    "yhat_ols_cf  = predict_centered(Xte, theta_ols_cf, y_mean)\n",
    "add_result(\"OLS closed-form\", theta_ols_cf, yhat_ols_cf, t0)\n",
    "\n",
    "# --- OLS: full-batch optimizers ---\n",
    "for opt in optimizers_full:\n",
    "    t0 = time.perf_counter()\n",
    "    theta = run_optimizer(Xtr, y_tr_c, lam=None, eta=eta_ols_full, iters=iters_full, optimizer=opt)\n",
    "    yhat  = predict_centered(Xte, theta, y_mean)\n",
    "    add_result(f\"OLS {opt}\", theta, yhat, t0)\n",
    "\n",
    "# --- OLS: mini-batch optimizers ---\n",
    "for opt in optimizers_mb:\n",
    "    t0 = time.perf_counter()\n",
    "    theta = sgd_minibatch(Xtr, y_tr_c, lam=0.0, epochs=epochs_mb, batch_size=batch_size,\n",
    "                          eta=eta_mb_ols, optimizer=opt, seed=seed_mb)\n",
    "    yhat  = predict_centered(Xte, theta, y_mean)\n",
    "    add_result(f\"OLS {opt} (mini-batch)\", theta, yhat, t0)\n",
    "\n",
    "# --- Ridge: closed form ---\n",
    "t0 = time.perf_counter()\n",
    "theta_ridge_cf = np.linalg.solve(Xtr.T @ Xtr + lam_ridge * np.eye(Xtr.shape[1]), Xtr.T @ y_tr_c)\n",
    "yhat_ridge_cf  = predict_centered(Xte, theta_ridge_cf, y_mean)\n",
    "add_result(\"Ridge closed-form\", theta_ridge_cf, yhat_ridge_cf, t0)\n",
    "\n",
    "# --- Ridge: full-batch optimizers ---\n",
    "for opt in optimizers_full:\n",
    "    t0 = time.perf_counter()\n",
    "    theta = run_optimizer(Xtr, y_tr_c, lam=lam_ridge, eta=eta_ridge_full, iters=iters_full, optimizer=opt)\n",
    "    yhat  = predict_centered(Xte, theta, y_mean)\n",
    "    add_result(f\"Ridge {opt}\", theta, yhat, t0)\n",
    "\n",
    "# --- Ridge: mini-batch optimizers ---\n",
    "for opt in optimizers_mb:\n",
    "    t0 = time.perf_counter()\n",
    "    theta = sgd_minibatch(Xtr, y_tr_c, lam=lam_ridge, epochs=epochs_mb, batch_size=batch_size,\n",
    "                          eta=eta_mb_ridge, optimizer=opt, seed=seed_mb)\n",
    "    yhat  = predict_centered(Xte, theta, y_mean)\n",
    "    add_result(f\"Ridge {opt} (mini-batch)\", theta, yhat, t0)\n",
    "\n",
    "# --- Lasso: scikit (coordinate descent) ---\n",
    "t0 = time.perf_counter()\n",
    "lasso = Lasso(alpha=lam_lasso, fit_intercept=False, max_iter=20000, tol=1e-4, random_state=42)\n",
    "lasso.fit(Xtr, y_tr_c)\n",
    "theta_lasso = lasso.coef_.ravel()\n",
    "yhat_lasso  = predict_centered(Xte, theta_lasso, y_mean)\n",
    "add_result(\"Lasso scikit\", theta_lasso, yhat_lasso, t0)\n",
    "\n",
    "# ---- Print leaderboard (sorted by MSE) ----\n",
    "results_sorted = sorted(results, key=lambda d: d[\"MSE\"])\n",
    "w = max(len(r[\"method\"]) for r in results_sorted)\n",
    "print(\"\\n\" + \"=\"*(w+44))\n",
    "print(\" LEADERBOARD (lower MSE is better) \")\n",
    "print(\"=\"*(w+44))\n",
    "print(f\"{'method'.ljust(w)}   MSE          R²        ||θ||₂       time[s]\")\n",
    "for r in results_sorted:\n",
    "    print(f\"{r['method'].ljust(w)}   {r['MSE']:.6f}   {r['R2']:.4f}   {r['||θ||2']:.3e}   {r['time_s']:.3f}\")\n",
    "\n",
    "best = results_sorted[0]\n",
    "print(\"\\nBest by MSE:\", best[\"method\"], f\"(MSE={best['MSE']:.6f}, R²={best['R2']:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
