{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b40ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 20) (40, 20)\n"
     ]
    }
   ],
   "source": [
    "# ---- imports & data ----\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, pathlib\n",
    "\n",
    "# Project-1 folder (parent of this Notebooks/ folder)\n",
    "proj_dir = pathlib.Path.cwd().parent\n",
    "if str(proj_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_dir))\n",
    "\n",
    "# Figures folder\n",
    "fig_dir = proj_dir / \"Figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from Code.data import make_data, build_features, split_and_scale\n",
    "from Code.metrics import mse, r2\n",
    "\n",
    "# Repro\n",
    "rng = np.random.default_rng(2025)\n",
    "\n",
    "# Data + features once\n",
    "X_raw, y_raw = make_data(n=200, noise_sd=0.1, seed=2025)\n",
    "Phi = build_features(X_raw, degree=20, include_bias=False)   # big basis; we'll slice p later\n",
    "\n",
    "# One split/scale (returns scaled features and centered y)\n",
    "X_tr_s, X_te_s, y_tr_c, y_te, scaler, y_mean = split_and_scale(Phi, y_raw, test_size=0.2, random_state=1, center_y=True)\n",
    "\n",
    "n_tr, p_full = X_tr_s.shape\n",
    "print(X_tr_s.shape, X_te_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a223a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_ols(X, y_c, theta):\n",
    "    # ∇ (1/2n)||y_c - Xθ||^2 = Xᵀ(Xθ - y_c)/n\n",
    "    r = X @ theta - y_c\n",
    "    return (X.T @ r) / X.shape[0]\n",
    "\n",
    "def grad_ridge(X, y_c, theta, lam):\n",
    "    # OLS grad + λθ (intercept not in θ; y is centered)\n",
    "    return grad_ols(X, y_c, theta) + lam * theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244205dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_full(X, y_c, theta0, eta, iters, grad_fn, **grad_kw):\n",
    "    \"\"\"Full-batch GD. Returns θ path and train-loss list.\"\"\"\n",
    "    theta = theta0.copy()\n",
    "    thetas, losses = [], []\n",
    "    for t in range(iters):\n",
    "        g = grad_fn(X, y_c, theta, **grad_kw)\n",
    "        theta -= eta * g\n",
    "        thetas.append(theta.copy())\n",
    "        losses.append(0.5*np.mean((y_c - X @ theta)**2))\n",
    "    return np.array(thetas), np.array(losses)\n",
    "\n",
    "def gd_full_opt(X, y_c, theta0, iters, grad_fn, opt=\"momentum\", eta=1e-2, **kw):\n",
    "    \"\"\"Full-batch GD with optimizer states.\"\"\"\n",
    "    theta = theta0.copy()\n",
    "    v = np.zeros_like(theta)         # momentum / rmsprop / adam first moment\n",
    "    s = np.zeros_like(theta)         # adagrad accumulator / rmsprop EMA / adam second moment\n",
    "    thetas, losses = [], []\n",
    "\n",
    "    eps = 1e-8\n",
    "    beta1, beta2 = 0.9, 0.999\n",
    "    for t in range(1, iters+1):\n",
    "        g = grad_fn(X, y_c, theta, **kw)\n",
    "\n",
    "        if opt == \"momentum\":\n",
    "            v = 0.9*v + g\n",
    "            step = eta * v\n",
    "\n",
    "        elif opt == \"adagrad\":\n",
    "            s += g*g\n",
    "            step = eta * g / (np.sqrt(s) + eps)\n",
    "\n",
    "        elif opt == \"rmsprop\":\n",
    "            s = 0.9*s + 0.1*(g*g)\n",
    "            step = eta * g / (np.sqrt(s) + eps)\n",
    "\n",
    "        elif opt == \"adam\":\n",
    "            v = beta1*v + (1-beta1)*g\n",
    "            s = beta2*s + (1-beta2)*(g*g)\n",
    "            v_hat = v / (1 - beta1**t)\n",
    "            s_hat = s / (1 - beta2**t)\n",
    "            step = eta * v_hat / (np.sqrt(s_hat) + eps)\n",
    "\n",
    "        else:  # plain GD\n",
    "            step = eta * g\n",
    "\n",
    "        theta -= step\n",
    "        thetas.append(theta.copy())\n",
    "        losses.append(0.5*np.mean((y_c - X @ theta)**2))\n",
    "    return np.array(thetas), np.array(losses)\n",
    "\n",
    "def sgd_minibatch(X, y_c, theta0, eta, iters, batch_size, grad_fn, **grad_kw):\n",
    "    \"\"\"Mini-batch SGD (no momentum for brevity; combine with gd_full_opt if you like).\"\"\"\n",
    "    theta = theta0.copy()\n",
    "    n = X.shape[0]\n",
    "    thetas, losses = [], []\n",
    "    for _ in range(iters):\n",
    "        idx = rng.choice(n, size=batch_size, replace=False)\n",
    "        Xb, yb = X[idx], y_c[idx]\n",
    "        g = grad_fn(Xb, yb, theta, **grad_kw)\n",
    "        theta -= eta * g\n",
    "        thetas.append(theta.copy())\n",
    "        losses.append(0.5*np.mean((y_c - X @ theta)**2))  # evaluate on full train\n",
    "    return np.array(thetas), np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726fc98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
