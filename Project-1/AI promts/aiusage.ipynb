{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c8e5c1-ba6b-4151-ba4a-d0e1107be5b0",
   "metadata": {},
   "source": [
    "Q: Looking at the project description and all the exercises; What is the best structure to have in the code? I don't need the code itself, but a structure and plan to follow for making the code\n",
    "\n",
    "Great question. Here’s a **simple, robust project structure** that maps *exactly* to the parts (a–h) in your brief, keeps leakage under control, and lets you reuse code cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Folder layout (minimal but scalable)\n",
    "\n",
    "```\n",
    "project1/\n",
    "├─ data/                      # (optional) saved arrays if you want\n",
    "├─ figures/                   # all plots saved here\n",
    "├─ results/                   # CSV/JSON of metrics per run\n",
    "├─ src/\n",
    "│  ├─ data.py                 # data generation + feature building + split/scale\n",
    "│  ├─ models.py               # OLS / Ridge / Lasso closed-form + predict helpers\n",
    "│  ├─ optim.py                # GD / Momentum / Adagrad / RMSprop / Adam / SGD\n",
    "│  ├─ resampling.py           # bootstrap, k-fold CV, bias-variance utilities\n",
    "│  ├─ metrics.py              # MSE, R2, L2-norm, condition number, etc.\n",
    "│  ├─ plots.py                # all plotting helpers (no math here)\n",
    "│  └─ utils.py                # seeds, config, reproducibility, saving, timers\n",
    "├─ notebooks/\n",
    "│  ├─ 01_OLS_degree.ipynb     # Part (a)\n",
    "│  ├─ 02_Ridge_lambda.ipynb   # Part (b)\n",
    "│  ├─ 03_GD_variants.ipynb    # Parts (c,d,e,f)\n",
    "│  ├─ 04_BiasVariance.ipynb   # Part (g)\n",
    "│  └─ 05_CrossValidation.ipynb# Part (h)\n",
    "└─ README.md\n",
    "```\n",
    "\n",
    "If you want **super-minimal**, you can collapse to just:\n",
    "\n",
    "* `src/models.py` (math & training),\n",
    "* `src/plots.py` (figures),\n",
    "* a couple of notebooks that import those.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Module responsibilities (what each file does)\n",
    "\n",
    "## `src/data.py`\n",
    "\n",
    "* `runge(x)`: the target function.\n",
    "* `make_data(n, noise_sd, seed)`: build `(x, y)`.\n",
    "* `build_features(x, degree, include_bias=False)`: polynomial design matrix.\n",
    "* `train_test_scale(X, y, test_size, random_state)`: **single split**, compute **train-only** mean/std, return:\n",
    "\n",
    "  * `X_tr, X_te, y_tr, y_te`\n",
    "  * `X_tr_s, X_te_s` (scaled with **train** stats)\n",
    "  * `y_mean, y_tr_c` (centered train targets)\n",
    "  * (Optional) a small `Config` (seed, degree, noise, etc.).\n",
    "\n",
    "**Why:** all leakage-sensitive steps live in one place; every experiment reuses the same safe split/scale pattern.\n",
    "\n",
    "## `src/models.py`\n",
    "\n",
    "Closed-form and “one-shot” solvers:\n",
    "\n",
    "* `fit_ols_closed(Xtr_s, ytr_c)` → `theta`.\n",
    "* `fit_ridge_closed(Xtr_s, ytr_c, lam, n_factor=True)` → `theta`\n",
    "  (`n_factor=True` solves $(XᵀX + nλI)θ = Xᵀy_c$; `False` uses $λI$ only. Document this!)\n",
    "* `predict_from_centered(Xs, theta, y_mean)` → `y_pred` (adds intercept back).\n",
    "* Convenience **sweeps**:\n",
    "\n",
    "  * `sweep_degree(X_full, y, deg_max)` → arrays of test MSE/R²/‖θ‖ vs degree (Part a).\n",
    "  * `sweep_ridge(X_full, y, degree, lambdas)` → test MSE/R²/‖θ‖ vs λ (Part b).\n",
    "\n",
    "**Why:** one import gives you everything you need for parts (a,b).\n",
    "\n",
    "## `src/optim.py`\n",
    "\n",
    "Gradient methods (Parts c–f):\n",
    "\n",
    "* Generic loop: `gd(Xtr_s, ytr_c, eta, iters, init=None)` → `theta_path`.\n",
    "* Add variants: `gd_momentum`, `adagrad`, `rmsprop`, `adam`, and `sgd` (mini-batch).\n",
    "* Shared gradients:\n",
    "\n",
    "  * OLS: $\\nabla = -\\tfrac{1}{n}Xᵀ(y - Xθ)$.\n",
    "  * Ridge: OLS gradient + $2λθ$ (or $λθ$ depending on your chosen convention). Keep it consistent with your `fit_ridge_closed`.\n",
    "\n",
    "**Why:** clean separation between *optimizers* and *models*. You can plug any gradient into any optimizer.\n",
    "\n",
    "## `src/resampling.py`\n",
    "\n",
    "* `bootstrap_predictions(model_fit_fn, predict_fn, Xtr_s, ytr_c, Xte_s, y_mean, B)` → `(B, n_test)` matrix of predictions.\n",
    "* `bias_variance_from_preds(P, y_true)` → bias², var, (and optional MSE) averaged over test points.\n",
    "* `kfold_cv(X_full, y, degree, k, model='ols'|'ridge', lam=None)` → mean test MSE across folds.\n",
    "\n",
    "**Why:** your bias–variance (Part g) and CV (Part h) use the same primitives, regardless of model.\n",
    "\n",
    "## `src/metrics.py`\n",
    "\n",
    "* `mse(y_true, y_pred)`, `r2(y_true, y_pred)`.\n",
    "* `l2_norm(theta)`, `condition_number(X)`.\n",
    "* (Optional) `noise_floor = sigma**2` helper for plots.\n",
    "\n",
    "**Why:** no repeated metric code, easy unit testing.\n",
    "\n",
    "## `src/plots.py`\n",
    "\n",
    "* `plot_mse_r2_vs_degree(degrees, mse, r2)`\n",
    "* `plot_ridge_curves(lambdas, mse, r2)`\n",
    "* `plot_theta_norms(x, norms, xlabel)`\n",
    "* `plot_bias_variance(degrees_or_ns, bias2, var, mse=None)`\n",
    "* stylistic knobs only—**no math** here.\n",
    "\n",
    "**Why:** figures stay consistent and one-liner to reproduce.\n",
    "\n",
    "## `src/utils.py`\n",
    "\n",
    "* `set_seed(seed)`, small `save_json`, `save_csv`, `timestamped_fname`, `ensure_dir`.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Notebook/script flow (what each notebook does)\n",
    "\n",
    "### 01\\_OLS\\_degree.ipynb — Part (a)\n",
    "\n",
    "1. `from src.data import make_data, build_features`\n",
    "2. Make data; build `X_full` (degree=15).\n",
    "3. `degrees, mse_deg, r2_deg, norms = sweep_degree(X_full, y, deg_max=15)`.\n",
    "4. Plot MSE/R² vs degree; plot ‖θ‖ vs degree; optionally print condition numbers.\n",
    "5. **Also**: “dependence on number of data points” → fix degree `p`, subsample training of size `m`, average test MSE/R² (reuse a small helper in `models.py` or write a tiny loop in the notebook, but keep split/scale logic from `data.py`).\n",
    "\n",
    "### 02\\_Ridge\\_lambda.ipynb — Part (b)\n",
    "\n",
    "1. Fix an “interesting” degree `p` (e.g., 10–12).\n",
    "2. `lambdas = logspace(...)`.\n",
    "3. `mse_r, r2_r, norms_r = sweep_ridge(X_full, y, degree=p, lambdas=lambdas)`.\n",
    "4. Plot MSE/R² vs λ (log x-axis), plot ‖θ‖ vs λ. Compare to OLS line.\n",
    "\n",
    "### 03\\_GD\\_variants.ipynb — Parts (c, d, e, f)\n",
    "\n",
    "1. Build split/scale once.\n",
    "2. For OLS and Ridge, run `gd` with different `eta` and `iters`, overlay convergence of MSE on train/test.\n",
    "3. Add momentum/Adagrad/RMSprop/Adam. Compare speed/stability.\n",
    "4. Implement LASSO gradient (subgradient on L1) and compare to ridge/OLS.\n",
    "5. Add `sgd` (mini-batch) and compare to full-batch GD.\n",
    "\n",
    "### 04\\_BiasVariance.ipynb — Part (g)\n",
    "\n",
    "1. Fix degree(s) and/or number of points.\n",
    "2. Use `bootstrap_predictions` to get prediction matrix `P`.\n",
    "3. Compute bias²/var via `bias_variance_from_preds(P, y_true)`; plot bias² & var vs degree and vs #points.\n",
    "4. Optionally confirm `MSE ≈ bias² + var + σ²`.\n",
    "\n",
    "### 05\\_CrossValidation.ipynb — Part (h)\n",
    "\n",
    "1. Implement k-fold CV for OLS, Ridge, Lasso (you can call `kfold_cv` or sklearn’s `KFold`).\n",
    "2. Compare CV MSE with bootstrap MSE; discuss differences.\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Conventions you should lock down (and mention in your report)\n",
    "\n",
    "* **Scaling & centering:** always fit mean/std on **train only**; center `y_train` (intercept added back to predictions).\n",
    "* **Ridge λ convention:** choose one definition and be consistent.\n",
    "\n",
    "  * If you minimize $(1/n)\\|y-Xθ\\|^2 + λ\\|θ\\|^2$, your closed-form is $(XᵀX + nλI)^{-1}Xᵀy_c$.\n",
    "  * If you minimize $\\|y-Xθ\\|^2 + λ\\|θ\\|^2$, it’s $(XᵀX + λI)^{-1}Xᵀy_c$.\n",
    "    Document which one you use; don’t mix in different parts.\n",
    "* **Reproducibility:** set seeds *once* (NumPy default\\_rng or `np.random.seed`).\n",
    "* **Fair comparisons:** use the **same split** and **same scaling stats** across degree/λ sweeps.\n",
    "* **Saving artifacts:** save CSVs of sweeps to `results/` and plots to `figures/` (one line in each notebook).\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Minimal function signatures (to guide your coding)\n",
    "\n",
    "```python\n",
    "# data.py\n",
    "def make_data(n=100, noise_sd=0.1, seed=42): ...\n",
    "def build_features(x, degree, include_bias=False): ...\n",
    "def train_test_scale(X, y, test_size=0.2, random_state=42): ...\n",
    "\n",
    "# models.py\n",
    "def fit_ols_closed(Xtr_s, ytr_c): ...\n",
    "def fit_ridge_closed(Xtr_s, ytr_c, lam, n_factor=True): ...\n",
    "def predict_from_centered(Xs, theta, y_mean): ...\n",
    "def sweep_degree(X_full, y, deg_max=15, test_size=0.2, random_state=42): ...\n",
    "def sweep_ridge(X_full, y, degree, lambdas, test_size=0.2, random_state=42, n_factor=True): ...\n",
    "\n",
    "# optim.py\n",
    "def grad_ols(Xs, y_c, theta): ...\n",
    "def grad_ridge(Xs, y_c, theta, lam, n_factor=True): ...\n",
    "def gd(Xs, y_c, grad_fn, eta, num_iters, theta0=None): ...\n",
    "# momentum/adagrad/rmsprop/adam/sgd variants ...\n",
    "\n",
    "# resampling.py\n",
    "def bootstrap_predictions(fit_fn, pred_fn, Xtr_s, ytr_c, Xte_s, y_mean, B=300): ...\n",
    "def bias_variance_from_preds(P, y_true): ...\n",
    "def kfold_cv(X_full, y, degree, k=5, model='ols', lam=None): ...\n",
    "\n",
    "# metrics.py\n",
    "def mse(y_true, y_pred): ...\n",
    "def r2(y_true, y_pred): ...\n",
    "def l2_norm(theta): ...\n",
    "def condition_number(X): ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Final tip\n",
    "\n",
    "Start with **part (a) OLS degree sweep** wired through `data.py`, `models.py`, and `plots.py`. Once that’s solid, **clone the pattern** for Ridge (part b), then add `optim.py` for gradient-based parts. Bias–variance and CV drop in naturally once the split/scale and predict conventions are consistent.\n",
    "\n",
    "If you want, I can turn this plan into stub files with docstrings you can fill in next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205412a-88b1-436f-a88a-db50b27581be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv – FYS-STK4155)",
   "language": "python",
   "name": "fys-stk4155-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
